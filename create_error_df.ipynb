{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import torch\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, INPUT_DIM, HIDDEN_DIM, DEVICE='cuda'):\n",
    "        super(FFN, self).__init__()\n",
    "        self.NAME = \"FFN\"\n",
    "        self.HIDDEN_DIM = HIDDEN_DIM\n",
    "        self.INPUT_DIM = INPUT_DIM\n",
    "        self.devivce = DEVICE\n",
    "\n",
    "\n",
    "        self.input = nn.Linear(self.INPUT_DIM, self.HIDDEN_DIM).to(self.devivce)\n",
    "        self.out = nn.Linear(self.HIDDEN_DIM, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X1 = torch.sigmoid(self.input(X))\n",
    "        X2 = self.out(X1)\n",
    "\n",
    "        y_hat = X2.view(-1)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def computeLoss(self, logits, labels):\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### FFN \n",
    "\n",
    "class FfnClf(BaseEstimator): # Inherits scikit-learn base classifier\n",
    "    '''Let's your model be used by LearningWithNoisyLabels'''\n",
    "    def __init__(self, input_size, hidden_size, num_classes, batch_size, device='cuda', learning_rate = 1e-2, num_epochs = 200, verbose=True):\n",
    "\n",
    "        self.device = device\n",
    "        self.model = FFN(input_size, hidden_size).to(self.device)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight = None):\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-3) #weight_decay=1e-3\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.99)\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), \n",
    "                                                       torch.tensor(y, dtype=torch.float32))\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        # --- train the model ---\n",
    "        total_step = len(train_loader)\n",
    "        training_hidden_list = []\n",
    "        training_images = []\n",
    "        loss_list = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "            final_loss = 0.0\n",
    "            for i, (X, y) in enumerate(train_loader):\n",
    "                X = X.to(self.device)\n",
    "\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                y = y.float()\n",
    "\n",
    "                # --- Forward pass ---\n",
    "                y_logit = self.model(X)\n",
    "                \n",
    "                # --- Backward and optimize ---\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.computeLoss(y_logit, y.float())\n",
    "                final_loss += loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.verbose:\n",
    "                    if (i+1) % 1 == 0:\n",
    "                        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, self.num_epochs, i+1, total_step, loss.item()))\n",
    "            loss_list.append(final_loss/float(i))\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        y = copy.copy(X)\n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), \n",
    "                                                       torch.tensor(y, dtype=torch.float32))\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            data = []\n",
    "            preds = []\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(self.device)\n",
    "\n",
    "                y_logit = self.model(X)\n",
    "                y_pred_prob = torch.sigmoid(y_logit)\n",
    "\n",
    "                y_pred = np.round(y_pred_prob.cpu())\n",
    "                preds.append(y_pred)\n",
    "            preds = torch.cat(preds).numpy()\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y = copy.copy(X)\n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), \n",
    "                                                       torch.tensor(y, dtype=torch.float32))\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            data = []\n",
    "            preds = []\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(self.device)\n",
    "\n",
    "                y_logit = self.model(X)\n",
    "                y_pred_prob = torch.sigmoid(y_logit)\n",
    "                y_pred = y_pred_prob.cpu()\n",
    "                preds.append(y_pred)\n",
    "            preds = torch.cat(preds).numpy()\n",
    "            preds = np.hstack(((1. - preds).reshape(-1,1),preds.reshape(-1,1)))\n",
    "            print(preds.shape)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Beach','Sunset','FallFoliage','Field','Mountain','Urban']\n",
    "\n",
    "input_features = ['Att' + str(i) for i in range(1,295)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'scene'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Beach',\n",
      "'Sunset',\n",
      "'FallFoliage',\n",
      "'Field',\n",
      "'Mountain',\n",
      "'Urban',\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = load_scene(partition='Train')\n",
    "train_labels = make_binary(train_labels, classes)\n",
    "for c in train_labels.columns:\n",
    "    print(\"'\" + c +\"',\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Device configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Hyper-parameters ---\n",
    "input_size = len(input_features)\n",
    "hidden_size = 100\n",
    "n_hidden = 1\n",
    "num_classes = len(classes)\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_df = train_data[input_features]\n",
    "train_ls = train_labels[classes]\n",
    "\n",
    "cutoff = math.floor(train_df.shape[0]/2.)\n",
    "first_half_data = train_df[:cutoff]\n",
    "last_half_data = train_df[cutoff:]\n",
    "first_half_ls = train_ls[:cutoff]\n",
    "last_half_ls = train_ls[cutoff:]\n",
    "\n",
    "clf = BinaryRelevance(classifier=FfnClf(input_size, hidden_size, num_classes, batch_size, device, learning_rate, num_epochs, verbose=False))\n",
    "clf.fit(first_half_data, first_half_ls)\n",
    "\n",
    "preds_last_half = clf.predict(last_half_data.values)\n",
    "\n",
    "\n",
    "clf = BinaryRelevance(classifier=FfnClf(input_size, hidden_size, num_classes, batch_size, device, learning_rate, num_epochs, verbose=False))\n",
    "clf.fit(last_half_data, last_half_ls)\n",
    "\n",
    "preds_first_half = clf.predict(first_half_data.values)\n",
    "\n",
    "preds_first = np.asarray(preds_first_half.todense())\n",
    "preds_last = np.asarray(preds_last_half.todense())\n",
    "preds_combined = np.vstack((preds_first, preds_last))\n",
    "errors = train_ls.values - preds_combined\n",
    "error_df = pd.DataFrame(errors, columns = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.to_pickle('error_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
